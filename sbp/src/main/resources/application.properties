spring.application.name=sbp
management.metrics.tags.application=sbp
logging.config=classpath:logback-spring.xml

# Actuator / Health
management.endpoints.web.exposure.include=health,info,prometheus
management.endpoint.health.show-details=always
# Prometheus
management.endpoint.prometheus.access=unrestricted

# curl 'http://localhost:8080/actuator/prometheus' -i -X GET -H 'Accept: application/openmetrics-text; version=1.0.0; charset=utf-8'
# to get trace/span id
management.metrics.distribution.percentiles-histogram.http.server.requests=true
management.metrics.distribution.percentiles-histogram.http.client.requests=true


# Postgres DB
spring.datasource.url=jdbc:postgresql://localhost:5432/sbp
spring.datasource.username=admin
spring.datasource.password=
spring.jpa.hibernate.ddl-auto=none
spring.jpa.open-in-view=false
spring.datasource.driver-class-name=org.postgresql.Driver

# --- Hikari pool knobs (so logs show concrete values) ---
spring.datasource.hikari.pool-name=hikari-pool-sbp
spring.datasource.hikari.minimum-idle=5
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.auto-commit=false
spring.datasource.hikari.transaction-isolation=TRANSACTION_READ_COMMITTED
# optional: faster fail on bad endpoints
spring.datasource.hikari.connection-timeout=10000
spring.datasource.hikari.validation-timeout=5000

# Tell Hibernate we disabled auto-commit at the pool
spring.jpa.properties.hibernate.connection.provider_disables_autocommit=true

# (optional) show SQL to validate behavior
logging.level.org.hibernate.SQL=DEBUG
logging.level.org.hibernate.type.descriptor.sql=TRACE

# RabbitMQ
spring.rabbitmq.host=localhost
spring.rabbitmq.port=5672
spring.rabbitmq.username=admin
spring.rabbitmq.password=

# Redis
spring.data.redis.host=localhost
spring.data.redis.port=6379
spring.data.redis.username=
spring.data.redis.password=

# Trace
management.tracing.enabled=true
# Optional: Adjust sampling probability (1.0 means 100% of traces are sent)
management.tracing.sampling.probability=1.0

management.otlp.metrics.export.enabled=false
management.otlp.tracing.export.enabled=true
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces
management.otlp.tracing.transport=http

# Logging
#logging.pattern.level=trace_id=%mdc{traceId} span_id=%mdc{spanId} %p

# Kafka
spring.kafka.bootstrap-servers=${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
spring.kafka.properties.socket.connection.setup.timeout.ms=10000
spring.kafka.properties.request.timeout.ms=15000
spring.kafka.properties.retry.backoff.ms=250

# Producer Configuration
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.batch-size=65536
spring.kafka.producer.compression-type=lz4

# For JSON payloads (optional)
spring.kafka.producer.properties.spring.json.add.type.headers=true

# Consumer Configuration
spring.kafka.consumer.group-id=${KAFKA_GROUP_ID:sbp-local}
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.isolation-level=read_committed

# Listener Container
spring.kafka.listener.ack-mode=record
spring.kafka.listener.concurrency=3